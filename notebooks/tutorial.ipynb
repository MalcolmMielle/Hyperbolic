{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# Hyperbolic Learning in Action: Practice\n",
        "\n",
        "In this notebook, we are going to train, evaluate, and compare three Convolutional Neural Networks (CNNs):\n",
        "\n",
        "1. an ordinary, fully Euclidean one;\n",
        "2. one with the last layer in hyperbolic space;\n",
        "3. a fully hyperbolic network.\n",
        "\n",
        "We will use:\n",
        "\n",
        "- the CIFAR-10 and CIFAR-100 datasets, whereas the first is chosen for its simplicity and the second because it exhibits *hierarchical* structure;\n",
        "- the hyperbolic learning library `HypLL` for the hyperbolic layers, due to its ease of use.\n",
        "\n",
        "We will visualize data representations in the Euclidean and hyperbolic space.\n",
        "\n",
        "## Setup\n",
        "\n",
        "**If you are on Colab or Kaggle, get GPU acceleration and ensure GitHub can be reached**\n",
        "- Colab:\n",
        "    1. Click on the dropdown arrow on the right of the menu bar above the notebook, next to \"Connect\".\n",
        "    2. Select \"Change runtime type\".\n",
        "    3. Choose \"T4 GPU\" under \"Hardware accelerator\".\n",
        "- Kaggle:\n",
        "    1. Expand the section \"Session options\" on the right menu sidebar.\n",
        "    2. Select \"GPU P100\" under \"Accelerator\".\n",
        "    3. Just below, toggle the option \"Internet on\".\n",
        "\n",
        "### Environment\n",
        "\n",
        "Check if the notebook is already in the code repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "path_parts = os.getcwd().split(os.sep)\n",
        "repository_path = \"\"\n",
        "try:\n",
        "    repository_index = path_parts.index(\"hyperbolic-learning-tutorial-code\")\n",
        "    repository_path = os.sep.join(path_parts[: repository_index + 1])\n",
        "except ValueError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "id": "2"
      },
      "source": [
        "Get the repository if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3",
        "outputId": "10497987-0dee-44d3-abf3-53ea057f35e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hyperbolic-learning-tutorial-code'...\n",
            "remote: Enumerating objects: 165, done.\u001b[K\n",
            "remote: Counting objects: 100% (165/165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 165 (delta 70), reused 148 (delta 57), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (165/165), 31.65 KiB | 2.88 MiB/s, done.\n",
            "Resolving deltas: 100% (70/70), done.\n",
            "/content/hyperbolic-learning-tutorial-code\n"
          ]
        }
      ],
      "source": [
        "if repository_path == \"\":\n",
        "    !git clone https://github.com/Digital-Dermatology/hyperbolic-learning-tutorial-code.git\n",
        "    %cd hyperbolic-learning-tutorial-code\n",
        "    repository_path = \"hyperbolic-learning-tutorial-code\"\n",
        "else:\n",
        "    %cd {repository_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "source": [
        "Install requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5",
        "outputId": "2b158da0-faca-4c83-d091-94b105b54855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.0.1\n",
            "Collecting hypll==0.1.1 (from -r requirements.txt (line 1))\n",
            "  Downloading hypll-0.1.1-py3-none-any.whl.metadata (664 bytes)\n",
            "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pandas~=2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: plotly==5.24.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn~=1.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.5.1+cu124)\n",
            "Collecting torchinfo==1.8.0 (from -r requirements.txt (line 8))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchmetrics==1.6.1 (from -r requirements.txt (line 9))\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
            "Collecting umap-learn==0.5.7 (from -r requirements.txt (line 12))\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly==5.24.1->-r requirements.txt (line 5)) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly==5.24.1->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->-r requirements.txt (line 7)) (1.13.1)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics==1.6.1->-r requirements.txt (line 9))\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1->-r requirements.txt (line 10)) (11.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from umap-learn==0.5.7->-r requirements.txt (line 12)) (1.13.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn==0.5.7->-r requirements.txt (line 12)) (0.61.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn==0.5.7->-r requirements.txt (line 12))\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r requirements.txt (line 2)) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r requirements.txt (line 2)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.10.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.2->-r requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.2->-r requirements.txt (line 4)) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.6.1->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.6.1->-r requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics==1.6.1->-r requirements.txt (line 9)) (75.1.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn==0.5.7->-r requirements.txt (line 12)) (0.44.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->-r requirements.txt (line 7)) (3.0.2)\n",
            "Downloading hypll-0.1.1-py3-none-any.whl (34 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "Installing collected packages: torchinfo, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, umap-learn, torchmetrics, hypll\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed hypll-0.1.1 lightning-utilities-0.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pynndescent-0.5.13 torchinfo-1.8.0 torchmetrics-1.6.1 umap-learn-0.5.7\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "source": [
        "Add the project's root to the Python path for custom functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(os.path.join(repository_path, \"src\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "source": [
        "Set the `torch` device and seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from src.utils.torch_utils import get_available_device, set_seeds\n",
        "\n",
        "device = torch.device(get_available_device())\n",
        "set_seeds(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "source": [
        "### Data\n",
        "\n",
        "Get the datasets.\n",
        "\n",
        "Since this is a demonstration, and it does not use hyperparameter tuning, it is ok to work only with one split for training and one for evaluation, i.e. testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11",
        "outputId": "8a7a2eaf-dad2-4fa4-fa4d-ad39099ae6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 32.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Classes in the dataset: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "transform = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=(0.5, 0.5, 0.5),\n",
        "            std=(0.5, 0.5, 0.5),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"data\", train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "classes = train_dataset.classes\n",
        "assert test_dataset.classes == classes\n",
        "num_classes = len(classes)\n",
        "print(f\"Classes in the dataset: {classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "source": [
        "Prepare the data loaders.\n",
        "\n",
        "The batch size and the number of workers may be adjusted as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "num_workers = 0\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
        ")\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "## Euclidean Network\n",
        "\n",
        "### Architecture\n",
        "\n",
        "Start with a simple Euclidean convolutional network.\n",
        "\n",
        "To compare with hyperbolic networks without too much pain:\n",
        "\n",
        "- it has no batch normalization nor skip connections;\n",
        "- fully connected layers are used at the end instead of e.g. global pooling;\n",
        "- no transfer learning is used.\n",
        "\n",
        "Right before classification, reduce the input dimension to 2 to enable embedding visualization.\n",
        "This will lead to poor performance in Euclidean space, but it will be less of a problem for hyperbolic networks.\n",
        "The constraint can be relaxed if a dimensionality reduction method such as PCA, t-SNE, or UMAP is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15",
        "outputId": "9277ea51-14d4-4b4f-c44c-05714f5f70d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (4): ReLU()\n",
              "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (7): ReLU()\n",
              "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (9): Flatten(start_dim=1, end_dim=-1)\n",
              "  (10): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (11): ReLU()\n",
              "  (12): Linear(in_features=128, out_features=32, bias=True)\n",
              "  (13): ReLU()\n",
              "  (14): Linear(in_features=32, out_features=2, bias=True)\n",
              "  (15): Linear(in_features=2, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from torch.nn import Conv2d, Flatten, Linear, MaxPool2d, ReLU, Sequential\n",
        "\n",
        "last_channels = 3\n",
        "conv_channels = (32, 64, 128)\n",
        "fc_channels = (128, 32, 2)\n",
        "image_size = (32, 32)\n",
        "pool_kernel_size = 2\n",
        "pool_stride = 2\n",
        "conv_kernel_size = 3\n",
        "\n",
        "pool = MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
        "activation = ReLU()\n",
        "current_image_size = torch.tensor(image_size)\n",
        "layers = []\n",
        "for channels in conv_channels:\n",
        "    layers.append(\n",
        "        Conv2d(in_channels=last_channels, out_channels=channels, kernel_size=3)\n",
        "    )\n",
        "    current_image_size -= conv_kernel_size - 1\n",
        "    layers.append(activation)\n",
        "    layers.append(pool)\n",
        "    current_image_size //= pool_stride\n",
        "    last_channels = channels\n",
        "layers.append(Flatten())\n",
        "last_channels *= current_image_size.prod()\n",
        "for channels in fc_channels:\n",
        "    layers.append(\n",
        "        Linear(in_features=last_channels, out_features=channels)\n",
        "    )\n",
        "    layers.append(activation)\n",
        "    last_channels = channels\n",
        "layers = layers[:-1]  # remove the last activation\n",
        "layers.append(Linear(in_features=last_channels, out_features=len(classes)))\n",
        "euclidean_network = Sequential(*layers)\n",
        "euclidean_network = euclidean_network.to(device)\n",
        "euclidean_network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    for data, labels in test_dataloader:\n",
        "        outputs = euclidean_network(data.to(device))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17",
        "outputId": "20b59823-cb6b-4c1c-dd1a-f001c04e0f92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18",
        "outputId": "ddd7131c-45c3-47d1-9bed-e3014fbe414d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "Sequential                               --\n",
              "├─Conv2d: 1-1                            896\n",
              "├─ReLU: 1-2                              --\n",
              "├─MaxPool2d: 1-3                         --\n",
              "├─Conv2d: 1-4                            18,496\n",
              "├─ReLU: 1-5                              --\n",
              "├─MaxPool2d: 1-6                         --\n",
              "├─Conv2d: 1-7                            73,856\n",
              "├─ReLU: 1-8                              --\n",
              "├─MaxPool2d: 1-9                         --\n",
              "├─Flatten: 1-10                          --\n",
              "├─Linear: 1-11                           65,664\n",
              "├─ReLU: 1-12                             --\n",
              "├─Linear: 1-13                           4,128\n",
              "├─ReLU: 1-14                             --\n",
              "├─Linear: 1-15                           66\n",
              "├─Linear: 1-16                           30\n",
              "=================================================================\n",
              "Total params: 163,136\n",
              "Trainable params: 163,136\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(euclidean_network)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Define the metrics for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassMatthewsCorrCoef\n",
        "\n",
        "metrics = MetricCollection(\n",
        "    [\n",
        "        MulticlassAccuracy(num_classes=num_classes),\n",
        "        MulticlassMatthewsCorrCoef(num_classes=num_classes),\n",
        "    ]\n",
        ")\n",
        "metrics = metrics.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21",
      "metadata": {
        "id": "21"
      },
      "source": [
        "Evaluate before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "22",
      "metadata": {
        "id": "22"
      },
      "outputs": [],
      "source": [
        "def print_metrics(metrics: MetricCollection, prefix: str = \"\") -> None:\n",
        "    print(\n",
        "        prefix,\n",
        "        {k.replace(\"Multiclass\", \"\"): v.item() for k, v in metrics.compute().items()},\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23",
        "outputId": "4ac0b558-8a55-4310-ea84-5ac82c1e5685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics before training: {'Accuracy': 0.10000000149011612, 'MatthewsCorrCoef': 0}\n"
          ]
        }
      ],
      "source": [
        "metrics.reset()\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_dataloader:\n",
        "        outputs = euclidean_network(data.to(device))\n",
        "        metrics(outputs, labels.to(device))\n",
        "print_metrics(metrics, \"Metrics before training:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24",
      "metadata": {
        "id": "24"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25",
        "outputId": "8f78eaf5-97bf-49c9-8957-90a03b48eab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:16<00:00, 24.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.2123199999332428, 'MatthewsCorrCoef': 0.13437457382678986}\n",
            "Test:  {'Accuracy': 0.3272000253200531, 'MatthewsCorrCoef': 0.26243168115615845}\n",
            "Epoch 2 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:13<00:00, 28.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.3617199957370758, 'MatthewsCorrCoef': 0.29730042815208435}\n",
            "Test:  {'Accuracy': 0.412200003862381, 'MatthewsCorrCoef': 0.3570978045463562}\n",
            "Epoch 3 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:14<00:00, 27.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.4330199956893921, 'MatthewsCorrCoef': 0.37413567304611206}\n",
            "Test:  {'Accuracy': 0.46380001306533813, 'MatthewsCorrCoef': 0.40758007764816284}\n",
            "Epoch 4 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:14<00:00, 27.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.48256000876426697, 'MatthewsCorrCoef': 0.4276089668273926}\n",
            "Test:  {'Accuracy': 0.5026000142097473, 'MatthewsCorrCoef': 0.4497363567352295}\n",
            "Epoch 5 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:13<00:00, 28.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.5313800573348999, 'MatthewsCorrCoef': 0.48130208253860474}\n",
            "Test:  {'Accuracy': 0.5347999930381775, 'MatthewsCorrCoef': 0.4850894510746002}\n",
            "Epoch 6 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:13<00:00, 28.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.5748200416564941, 'MatthewsCorrCoef': 0.5291279554367065}\n",
            "Test:  {'Accuracy': 0.5730000138282776, 'MatthewsCorrCoef': 0.5274108052253723}\n",
            "Epoch 7 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:13<00:00, 28.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.6035999655723572, 'MatthewsCorrCoef': 0.5606430768966675}\n",
            "Test:  {'Accuracy': 0.58160001039505, 'MatthewsCorrCoef': 0.538537859916687}\n",
            "Epoch 8 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:13<00:00, 28.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.6241199970245361, 'MatthewsCorrCoef': 0.5835833549499512}\n",
            "Test:  {'Accuracy': 0.6003000140190125, 'MatthewsCorrCoef': 0.557213544845581}\n",
            "Epoch 9 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:13<00:00, 28.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.6444999575614929, 'MatthewsCorrCoef': 0.606185257434845}\n",
            "Test:  {'Accuracy': 0.5855000019073486, 'MatthewsCorrCoef': 0.545138418674469}\n",
            "Epoch 10 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:13<00:00, 28.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  {'Accuracy': 0.6613399982452393, 'MatthewsCorrCoef': 0.6251767873764038}\n",
            "Test:  {'Accuracy': 0.6172999739646912, 'MatthewsCorrCoef': 0.5763481855392456}\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "criterion.to(device)\n",
        "optimizer = Adam(euclidean_network.parameters(), lr=1e-3)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1} of {num_epochs}\")\n",
        "    metrics.reset()\n",
        "    for data, labels in tqdm(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = euclidean_network(data.to(device))\n",
        "        labels = labels.to(device)\n",
        "        metrics(outputs, labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print_metrics(metrics, \"Train: \")\n",
        "    metrics.reset()\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_dataloader:\n",
        "            outputs = euclidean_network(data.to(device))\n",
        "            metrics(outputs, labels.to(device))\n",
        "    print_metrics(metrics, \"Test: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {
        "id": "26"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "Get the embeddings to visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "27",
      "metadata": {
        "id": "27"
      },
      "outputs": [],
      "source": [
        "embeddings, predictions, labels = [], [], []\n",
        "with torch.no_grad():\n",
        "    for data, labels_batch in test_dataloader:\n",
        "        embeddings_batch = euclidean_network[:-1](data.to(device))\n",
        "        predictions_batch = euclidean_network[-1](embeddings_batch).argmax(dim=-1)\n",
        "        embeddings.append(embeddings_batch)\n",
        "        predictions.append(predictions_batch)\n",
        "        labels.append(labels_batch)\n",
        "embeddings = torch.cat(embeddings, dim=0)\n",
        "predictions = torch.cat(predictions, dim=0)\n",
        "labels = torch.cat(labels, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28",
      "metadata": {
        "id": "28"
      },
      "source": [
        "Save the plot to HTML to avoid overloading the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "29",
      "metadata": {
        "id": "29"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "df = pd.DataFrame(embeddings.cpu().numpy())\n",
        "df[\"prediction\"] = predictions.cpu().numpy()\n",
        "df[\"label\"] = [classes[i] for i in labels.cpu().numpy()]\n",
        "fig = px.scatter(data_frame=df, x=0, y=1, color=\"label\")\n",
        "fig.write_html(\"euclidean.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30",
      "metadata": {
        "id": "30"
      },
      "source": [
        "## Last hyperbolic layer\n",
        "\n",
        "Now it is time to roll up sleeves. Enjoy hacking!\n",
        "\n",
        "1. Define the hyperbolic manifold using [`hypll.manifolds.poincare_ball.PoincareBall`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.manifolds.poincare_ball.manifold.html) with a trainable curvature parameter [`hypll.manifolds.poincare_ball.Curvature`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.manifolds.poincare_ball.curvature.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "31",
      "metadata": {
        "id": "31"
      },
      "outputs": [],
      "source": [
        "from hypll.manifolds.poincare_ball import PoincareBall, Curvature\n",
        "\n",
        "curvature = Curvature(value=1.0, requires_grad=True)\n",
        "manifold = PoincareBall(c=curvature)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32",
      "metadata": {
        "id": "32"
      },
      "source": [
        "2. Starting with the Euclidean network, just before classification, lift the representation to hyperbolic space by constructing a [`hypll.tensors.TangentTensor`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.tensors.tangent_tensor.html) and using `PoincareBall`'s exponential map.\n",
        "    - Hint: you may also use the convenience layer [`src.layers.to_manifold.ToManifold`](https://github.com/Digital-Dermatology/hyperbolic-learning-tutorial-code/blob/main/src/layers/to_manifold.py) or take inspiration from it.\n",
        "3. Obtain the logits by replacing the linear classification layer of the Euclidean network with the calculation of the distances from (learned) hyperbolic hyperplanes.\n",
        "    - This operation, known as Hyperbolic Multinomial Logistic Regression, is implemented in [`src.layers.hmlr.HMLR`](https://github.com/Digital-Dermatology/hyperbolic-learning-tutorial-code/blob/main/src/layers/hmlr.py), feel free to use it directly or as a guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33",
      "metadata": {
        "id": "33"
      },
      "outputs": [],
      "source": [
        "last_hyperbolic_network = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34",
      "metadata": {
        "id": "34"
      },
      "source": [
        "4. Replace the Adam optimizer with Riemannian Adam from [`hypll.optim.RiemannianAdam`](https://hyperbolic-learning-library.readthedocs.io/en/latest/_autosummary/hypll.optim.adam.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35",
      "metadata": {
        "id": "35"
      },
      "outputs": [],
      "source": [
        "riemannian_optimizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36",
      "metadata": {
        "id": "36"
      },
      "source": [
        "5. Train the network for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37",
      "metadata": {
        "id": "37"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38",
      "metadata": {
        "id": "38"
      },
      "source": [
        "6. Visualize the embeddings with their labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39",
      "metadata": {
        "id": "39"
      },
      "outputs": [],
      "source": [
        "embeddings, predictions, labels = [], [], []\n",
        "with torch.no_grad():\n",
        "    for data, labels_batch in test_dataloader:\n",
        "        embeddings_batch = ...\n",
        "        predictions_batch = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40",
      "metadata": {
        "id": "40"
      },
      "outputs": [],
      "source": [
        "df = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41",
      "metadata": {
        "id": "41"
      },
      "source": [
        "7. Compare the training time, final performance, and representations with the euclidean ones!\n",
        "\n",
        "## Fully hyperbolic network\n",
        "\n",
        "Exercise 2:\n",
        "\n",
        "1. Define the hyperbolic manifold as in Exercise 1.\n",
        "2. Immediately after getting data from the `DataLoader`, lift it to the `PoincareBall` as in the previous exercise.\n",
        "3. Build a fully hyperbolic backbone using the layers `HLinear`, `HConv2D`, `HPool2D`, and `HReLU` from `hypll.nn`.\n",
        "4. Add the classification layer at the end using `src.layers.hmlr.HMLR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42",
      "metadata": {
        "id": "42"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "43",
      "metadata": {
        "id": "43"
      },
      "source": [
        "## Optional: CIFAR-100\n",
        "\n",
        "If you got this far, well done!!\n",
        "\n",
        "You should repeat the exercise with CIFAR-100, which has a more hierarchical structure, to see the benefits of hyperbolic learning for real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44",
      "metadata": {
        "id": "44"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}